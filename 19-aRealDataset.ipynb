{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1bed86b-9a42-4c42-aad8-2879c79cd642",
   "metadata": {},
   "source": [
    "# A Real Dataset\n",
    "\n",
    "- Fashion MNIST dataset :\n",
    "    - Training samples = 60,000 \n",
    "    - Testing samples = 10,000\n",
    "    - Sample size = 28x28\n",
    "    - #classes = 10 w/ each being a different clothing item\n",
    "    - Samples per class = 6,000 which balances the dataset\n",
    "- Common : \n",
    "    - Grayscale (go from 3-channel RGB values per pixel to a single Black to white range of 0 - 255 per px)\n",
    "    - Reisize images to normalize their dimensions\n",
    "    \n",
    "    \n",
    "- Notes :\n",
    "    - If dataset isn't already balanced, the NN $\\rightarrow$ biased to predict the class containing the most images. NN fundamentally seek out the steepest and quickest gradient descent to decrease loss, which might lead to a local minimum making the model unable to find the global loss minimum. May be best to trim samples from high-frequency classes in dataset. Other solutions are to 1) use class weights which needs to be validated in practice, 2) augment samples - crop, rotate, flip, etc.\n",
    "\n",
    "- Questions/Futher explore :\n",
    "    - cv2.IMREAD_UNCHANGED vs not using it and how the image is printing with plt.imshow()\n",
    "    - np.float32 and others\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f52918-df05-48e1-b5e8-492d281c8330",
   "metadata": {},
   "source": [
    "---\n",
    "## Data prep\n",
    "- Get from nnfs.io site\n",
    "- All CAPITAL VARIABLES bc in Python, these are CONSTANTS (formally known as IMMUTABLE)\n",
    "- pypi (bc they throw errors w/ conda installation) see w/ conda list : \n",
    "    - matplotlib=3.5.2\n",
    "    - numpy=1.21.6\n",
    "    - opencv-python=4.5.5.64\n",
    "    - pillow=9.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f47437f8-621d-4cfc-a0c4-d775077fe788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import os\n",
    "import urllib\n",
    "import urllib.request\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to ensure that my data matches tutorial's\n",
    "import nnfs\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f48f96-1779-42a7-b153-48904c170a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Download the compressed data (if the files is absent under the given path) \n",
    "using the urllib, a standard Py library\n",
    "'''\n",
    "\n",
    "URL = 'https://nnfs.io/datasets/fashion_mnist_images.zip'\n",
    "FILE = 'fashion_mnist_images.zip'\n",
    "FOLDER = 'fashion_mnist_images'\n",
    "\n",
    "\n",
    "# Unzip files \n",
    "if not os.path.isfile(FILE):\n",
    "    print(f'Downloading {URL} and saving as {FILE}...')\n",
    "    urllib.request.urlretrieve(URL, FILE)\n",
    "print('Unzipping images...') \n",
    "\n",
    "# with is a keyword that opens and close file\n",
    "with ZipFile(FILE) as zip_images: \n",
    "    zip_images.extractall(FOLDER)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d44175-53e2-4f81-ae98-8d823e1e27d3",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ada421a-7491-40ac-884a-2bb4349e429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = os.listdir('fashion_mnist_images/train')\n",
    "print('labels : ', labels)\n",
    "\n",
    "files = os.listdir('fashion_mnist_images/train/0')\n",
    "# print('\\ntotal #imgs in 0', len(files), '\\nall imgs in 0 : \\n', files)\n",
    "# print('\\nset of imgs in 0 : ', files[:10])\n",
    "\n",
    "image_data1 = cv2.imread('fashion_mnist_images/train/7/0002.png')\n",
    "# print(\"image_data1 : \", image_data1)\n",
    "\n",
    "# cv2.IMREAD_UNCHANGED, argument notifies the cv2 package that we intend to read in these images \n",
    "# in the same format as they were saved (grayscale in this case)\n",
    "image_data2 = cv2.imread('fashion_mnist_images/train/7/0002.png', cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "# see all characters in that one line that should be there; break new row\n",
    "np.set_printoptions(linewidth=200)\n",
    "print(\"image_data2 : \", np.shape(image_data2), type(image_data2), \"\\n\", image_data2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e2fb9d-73ec-4c82-8a4b-42ea8737f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da82e2fe-e36e-40ff-8f88-44a40e75ecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ed7121-7758-4b21-9ad5-d58a1cafcbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_data2, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d0f8187-025c-4542-b547-2fc307bc201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_dataset(dataset, path):\n",
    "    '''\n",
    "    Put samples into lists - input data (X) and targets in (y)\n",
    "    cv2.IMREAD_UNCHANGED, argument notifies the cv2 package that we intend to read in these images \n",
    "        in the same format as they were saved (grayscale in this case)\n",
    "    '''\n",
    "    # Scan all the directories and create a list of labels\n",
    "    labels = os.listdir(os.path.join(path, dataset))\n",
    "    # print(\"labels : \", labels)\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for label in labels:\n",
    "\n",
    "        images = os.listdir(os.path.join(path, dataset, label))\n",
    "        # print(\"images : \", images)\n",
    "        \n",
    "        for file in images:\n",
    "            get_file = os.path.join(path, dataset, label, file)\n",
    "            # print(\"get_file : \", get_file)\n",
    "\n",
    "            image = cv2.imread(get_file, cv2.IMREAD_UNCHANGED)\n",
    "            X.append(image)\n",
    "            y.append(label)\n",
    "            \n",
    "    return np.array(X), np.array(y).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8ec113a-ebdd-4ba3-971a-358acb667360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_mnist(path):\n",
    "    '''\n",
    "    Create and return our test and train data\n",
    "    '''\n",
    "    \n",
    "    X, y = load_mnist_dataset('train', path)\n",
    "    X_test, y_test = load_mnist_dataset('test', path)\n",
    "    \n",
    "    return X, y, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b9313b7-9f18-444a-bd7d-37a41bf4feac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X :  (60000, 28, 28)\n",
      "y :  (60000,)\n",
      "X_test :  (10000, 28, 28)\n",
      "y_test :  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# labels = fashion_mnist_images/train or test/ --> 0, 1, 2, ..., 9\n",
    "# images = fashion_mnist_images/train or test/label --> 1D list of imgs\n",
    "# get_file = fashion_mnist_images/train or test/label/file <-- returns\n",
    "\n",
    "np.set_printoptions(linewidth=200)\n",
    "path = 'fashion_mnist_images'\n",
    "X, y, X_test, y_test = create_data_mnist(path)\n",
    "\n",
    "# X, a 3D np array (#imgs, size in x, size in y) of the training images\n",
    "print(\"X : \", np.shape(X))\n",
    "\n",
    "# y, a 2D np array (#imgs, 1) of the training labels\n",
    "print(\"y : \", np.shape(y))\n",
    "\n",
    "# X_test, 3D array (#imgs, size in x, size in y) of testing images\n",
    "print(\"X_test : \", np.shape(X_test))\n",
    "\n",
    "# y_test, 2D array (#imgs, 1) of testing label\n",
    "print(\"y_test : \", np.shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2356a76-78a0-47a1-b0d2-85664b20472a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Data preprocessing by scaling\n",
    "\n",
    "- Scale the data (! the imgs, but the data representing them, the #s)\n",
    "- Img data range 0 to 255 (bc of grayscale values)\n",
    "    - But, NNs work best w/ range 0 to 1 or -1 to 1\n",
    "- Usually trial and error\n",
    "- Use np.float32 (a 32-bit float value) bc\n",
    "    - Currently uint8 (unsigned int, holds int values in range 0 to 255). If we keeep this, np will convert it to a float64 data type\n",
    "- Always run previous cell to get proper size, then run below; If !, new scaling may ! be -1 to 1\n",
    "- Dense layers\n",
    "    - Work on batches of 1D vectors\n",
    "    - Can! work on imgs shaped 28 x 28, 2D array so \n",
    "        - **flatten** imgs as in take every row of an img & append it to the 1st row of that array\n",
    "        - this will transfrom the 2D array $\\rightarrow$ into a 1D array (vector)\n",
    "    - 2D here will work w/ NN models called **convolutional neural networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d8d0850-9642-4f7a-a38c-2bcb2c9e30e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0 1.0 (60000, 28, 28) <class 'numpy.ndarray'>\n",
      "-1.0 1.0 (10000, 28, 28) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# X_64 = (X - 127.5) / 127.5\n",
    "# print(\"\\nX_64 : \", X.min(), X.max(), X_64.shape, type(X_64))\n",
    "\n",
    "\n",
    "# X_test_64 = (X - 127.5) / 127.5\n",
    "# print(\"\\nX_test_64 : \", X.min(), X.max(), X_test_64.shape, type(X_test_64))\n",
    "\n",
    "# print(\"----\\n\")\n",
    "\n",
    "X = (X.astype(np.float32) - 127.5) / 127.5\n",
    "print(X.min(), X.max(), X.shape, type(X))\n",
    "\n",
    "\n",
    "X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n",
    "print(X_test.min(), X_test.max(), X_test.shape, type(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f669b89-8c46-4328-93a9-b3a0d9996209",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Flatten ex\n",
    "'''\n",
    "ex = np.array([\n",
    "              [1, 2], \n",
    "              [3, 4]\n",
    "              ])\n",
    "print(\"ex : \\n\", ex)\n",
    "\n",
    "flattened = ex.reshape(-1)\n",
    "print(\"flattened : \\n\", flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a00a99a5-b434-4adb-9b0c-2ec045069500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X :  (60000, 784)\n",
      "X_test :  (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Real flatten\n",
    "- Don't use np.flatten() bc our intentions differs w/ a batch of samples\n",
    "- 28 * 28 = 784\n",
    "- \"-1\" as second dim means that we want to pull all of the sample data in this single dim into a 1D array\n",
    "'''\n",
    "X_row = X.shape[0]\n",
    "X = X.reshape(X_row, -1)\n",
    "print(\"X : \", X.shape)\n",
    "\n",
    "X_test_row = X_test.shape[0]\n",
    "X_test = X_test.reshape(X_test_row, -1)\n",
    "print(\"X_test : \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5fc7e8-75d9-49d7-b26e-32416f6f79f3",
   "metadata": {},
   "source": [
    "# Data shuffling\n",
    "\n",
    "- Bc samples and their targets are in order\n",
    "- If don't do this, NN will be bias towards the 1st class (9s)\n",
    "- Organize in order of chunks of 6,000 samples per label\n",
    "- Shuffle the sample and target arrays the same; otherwise, we'll have a very confused (and wrong) model as labels will no longer match samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "895f1379-53ce-4f6b-bd02-d51e63d7a5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 7 1 5 4 7 1 1 6 0 4 5 8 0 3 8 9 1 2 9 5 2 1 1 7 7 2 5 0 5 6 6 7 8 5 5 1 8 0 9 8 9 8 7 5 3 8 5 0 0 0 5 6 3 9 7 4 6 5 0 0 1 7 0 2 6 6 3 3 1 9 1 5 3 6 8 4 9 5 0 5 9 9 2 1 6 7 3 9 9 8 9 4 7 1 1 6 8 9\n",
      " 0]\n",
      "[0 7 2 4 6 2 4 3 2 2 9 8 6 2 3 5 4 7 0 9 2 4 3 8 0 8 1 9 4 8 1 2 6 2 0 8 7 4 8 7 3 5 8 4 6 8 5 7 2 6 3 1 0 9 2 6 3 5 6 2 5 4 3 3 3 8 4 1 1 7 8 5 4 2 9 7 8 2 9 0 8 5 5 4 5 9 0 7 7 6 9 4 7 8 6 5 6 2 1\n",
      " 3]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[26730 32284 31744 22016 34180 31607 33933 29785 24152 41213]\n",
      "[[-1. -1. -1. ... -1. -1. -1.]\n",
      " [-1. -1. -1. ... -1. -1. -1.]\n",
      " [-1. -1. -1. ... -1. -1. -1.]\n",
      " ...\n",
      " [-1. -1. -1. ... -1. -1. -1.]\n",
      " [-1. -1. -1. ... -1. -1. -1.]\n",
      " [-1. -1. -1. ... -1. -1. -1.]]\n",
      "[0 0 5 1 6 3 1 9 9 6 4 8 1 6 8]\n"
     ]
    }
   ],
   "source": [
    "# no shuffle so same order\n",
    "print(y[0:100])\n",
    "print(y[6000:6100])\n",
    "\n",
    "# the same for samples and targets\n",
    "keys = np.array(range(X.shape[0]))\n",
    "print(keys[:10])\n",
    "\n",
    "# shuffle so diff order\n",
    "np.random.shuffle(keys)\n",
    "print(keys[:10])\n",
    "\n",
    "# new order of indexes\n",
    "X = X[keys]\n",
    "print(X[:15])\n",
    "\n",
    "y = y[keys]\n",
    "print(y[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de6f8eec-9958-48cf-a5c6-81fc112b583d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Check reshuffle\n",
    "'''\n",
    "\n",
    "plt.imshow((X[10].reshape(28, 28)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c296883e-2d1f-4154-be45-78a893ce5f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Check the class at the same index as the above img\n",
    "Boot/Shoes class 9 (see by going to folder) & it prints class 9, so good\n",
    "Long sleeve/Jacket/Coat class 4 (see by going to folder) & it prints class 4, so good\n",
    "\n",
    "'''\n",
    "\n",
    "print(y[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4550ec4-4420-46e5-acfd-9a7424b7b5f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
